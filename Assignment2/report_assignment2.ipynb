{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Author**: Supreeth Suresh\n",
    "**Date**: 3/7/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Question 1](./images/Q1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    "\n",
    "states = (A, B, C)\n",
    "actions = (ab, ba, bc, ca, cb)\n",
    "V1(s) = (2, 2, 2)\n",
    "gamma = 0.5\n",
    "policy = uniform random policy \n",
    "\n",
    "$$ V_k(s) = \\sum_{a \\in A} \\pi(a|s) (r + \\gamma \\sum_{s' in S} p(s'|s,a) * V_{k-1}(s')) $$\n",
    "\n",
    "## Calculation\n",
    "\n",
    "[Hand written notes of the calculation](./pdf_handwritten/Q1.pdf)\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7.0, 1.0, 7.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "states = [\"A\", \"B\", \"C\"]\n",
    "actions = [\"ab\", \"bc\", \"ba\", \"ca\", \"cb\"]\n",
    "transitions_reward = {\n",
    "    \"A\": {\"ab\": -8},\n",
    "    \"B\": {\"ba\": 2, \"bc\": -2},\n",
    "    \"C\": {\"cb\": 8, \"ca\": 4} \n",
    "}\n",
    "transitions_prob = {\n",
    "    \"A\": {\"ab\": 1},\n",
    "    \"B\": {\"ba\": 1, \"bc\": 1},\n",
    "    \"C\": {\"cb\": 1, \"ca\": 0.25, \"cc\": 0.75}\n",
    "}\n",
    "\n",
    "gamma = 0.5\n",
    "\n",
    "V1 = [2, 2, 2]\n",
    "\n",
    "V2A = 1 * (transitions_reward[\"A\"][\"ab\"] + gamma * transitions_prob[\"A\"][\"ab\"] * V1[0])\n",
    "V2B = 0.5 * (transitions_reward[\"B\"][\"ba\"] + gamma * transitions_prob[\"B\"][\"ba\"] * V1[1]) + \\\n",
    "    0.5 * (transitions_reward[\"B\"][\"bc\"] + gamma * transitions_prob[\"B\"][\"bc\"] * V1[1])\n",
    "V2C = 0.5 * (transitions_reward[\"C\"][\"cb\"] + gamma * transitions_prob[\"C\"][\"cb\"] * V1[2]) + \\\n",
    "    0.5 * (transitions_reward[\"C\"][\"ca\"] + gamma * (transitions_prob[\"C\"][\"ca\"] * V1[2] + transitions_prob[\"C\"][\"cc\"] * V1[2]))\n",
    "V2 = [V2A, V2B, V2C]\n",
    "V2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Question 2](./images/Q2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hand Written notes for Problem 2](./pdf_handwritten/Q2.pdf)\n",
    "\n",
    "### 1\n",
    "State s and Action a belong to {-1, 0, 1} <br> <br>\n",
    "Feature vector $\\phi (s,a) = \\begin{bmatrix} 2 . s \\\\ a \\\\ 0.5 \\end{bmatrix}$ <br><br>\n",
    "\n",
    "Weight vector $W = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix}$\n",
    "<br>\n",
    "\n",
    "Q value: $q(s,a;w)$ is Linear combination of Weight and Feature vector\n",
    "<br>\n",
    "\n",
    "$$q(s,a;w) = W^T . \\phi (s,a) $$\n",
    "\n",
    "So, \n",
    "<br>\n",
    "$$q(s,a;w) =  w_o . (2 . s) + w_1 . a + w_2 . 0.5 $$\n",
    "$$q(s,a;w) =  2sw_o + aw_1 + 0.5w_2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Question 2](./images/Q2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "After single sample (s,a,r,s') <br><br>\n",
    "Assuming weight vector of target network <br><br>\n",
    "Target Weight vector $W^- = \\begin{bmatrix} w^-_0 \\\\ w^-_1 \\\\ w^-_2 \\end{bmatrix}$ <br> <br>\n",
    "loss_fn is obtained using TD error\n",
    "<br><br>\n",
    "So, TD target $y = r + \\gamma \\max_{a'} q(s',a'; w^-)$\n",
    "<br><br>\n",
    "$  TD Error = (y - q(s,a;w))$\n",
    "\n",
    "The objective here is to minimize the Mean Squared TD Error. <br><br>\n",
    "So the objective function is, \n",
    "\n",
    "$ J(w) = MSE(Target Q - Current Q) $\n",
    "\n",
    "$ J(w) = 0.5 * (y - q(s,a;w))^2 $\n",
    "\n",
    "$ J(w) = 0.5 * ((r + \\gamma \\max_{a'} q(s',a'; w^-)) - q(s,a;w))^2 $\n",
    "\n",
    "Minimize J(w).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3\n",
    "Weight vector $W = \\begin{bmatrix} -2 \\\\ 1 \\\\ -1 \\end{bmatrix}$ <br> <br>\n",
    "Target Weight vector $W^- = \\begin{bmatrix} -1 \\\\ 2 \\\\ 1 \\end{bmatrix}$ <br> <br>\n",
    "\n",
    "Sample ((s = 1, a = 0, r = 2, s' = 2)) <br><br>\n",
    "Learning rate $\\alpha = 0.2$, Assuming $\\gamma = 0.9$\n",
    "\n",
    "1. Calculate current $q(s,a;w) = W^T \\phi (s,a)$\n",
    "<br><br>\n",
    "$q(s,a;w) =  2sw_o + aw_1 + 0.5w_2 $ <br> <br>\n",
    "Substituting s=1, a=0, <br><br>\n",
    "$q(s,a;w) = (2*1*-2) + (0*1) + (0.5*-1) $\n",
    "$q(s,a;w) = -4.5 $\n",
    "\n",
    "2. Calculate TD Target <br><br>\n",
    "$y = r + \\gamma \\max_{a'} q(s',a'; w^-)$ <br><br>\n",
    "For this case, next state s'=2 and next action is taken from the action space $a = {-1,0,1}$ <br><br>\n",
    "$\\phi(s,a) = [2*s', a', 0.5]$ and $W^- = \\begin{bmatrix} -1 \\\\ 2 \\\\ 1 \\end{bmatrix}$ <br><br>\n",
    "$q(s',a'; w^-) = W^T \\phi(s,a)$ <br><br>\n",
    "    a. When s' = 2, a' = -1 <br>\n",
    "    $q(s',a'; w^-) =[-1, 2, 1] \\begin{bmatrix} 2*s' \\\\ a' \\\\ 0.5 \\end{bmatrix}$ <br>\n",
    "    $q(s',a'; w^-) =[-1, 2, 1] \\begin{bmatrix} 2*2 \\\\ -1 \\\\ 0.5 \\end{bmatrix}$ <br><br>\n",
    "    $q(s',a'; w^-) = -5.5 $\n",
    "    <br><br>\n",
    "    b. When s' = 2, a' = 0 <br>\n",
    "    $q(s',a'; w^-) =[-1, 2, 1] \\begin{bmatrix} 2*s' \\\\ a' \\\\ 0.5 \\end{bmatrix}$ <br>\n",
    "    $q(s',a'; w^-) =[-1, 2, 1] \\begin{bmatrix} 2*2 \\\\ 0 \\\\ 0.5 \\end{bmatrix}$ <br><br>\n",
    "    $q(s',a'; w^-) = -3.5 $\n",
    "    <br><br>\n",
    "    c. When s' = 2, a' = 1 <br>\n",
    "    $q(s',a'; w^-) =[-1, 2, 1] \\begin{bmatrix} 2*s' \\\\ a' \\\\ 0.5 \\end{bmatrix}$ <br>\n",
    "    $q(s',a'; w^-) =[-1, 2, 1] \\begin{bmatrix} 2*2 \\\\ 1 \\\\ 0.5 \\end{bmatrix}$ <br><br>\n",
    "    $q(s',a'; w^-) = -1.5 $\n",
    "    <br><br>\n",
    "    The action that yields the maximum q here is when s'= 2 and a'= 1 <br><br>\n",
    "\n",
    "    So, <br>\n",
    "    $y = r + \\gamma \\max_{a'} q(s',a'; w^-)$ <br><br>\n",
    "    $y = 2 + 0.9 * - 1.5$ <br><br>\n",
    "    $y = 0.65 $\n",
    "\n",
    "3. Calculate TD Error <br><br>\n",
    "$TD Error = y - q(s,a;w)$<br>\n",
    "$TD Error = 0.65 - (-4.5)$<br>\n",
    "$TD Error = 5.15 $\n",
    "<br> <br>\n",
    "\n",
    "4. Objective function J(w) and its gradient is calculated as follows <br><br>\n",
    "    a = 0, s = 1 <br><br>\n",
    "    $ J(w) = 0.5 * (y - q(s,a;w))^2 $ and <br><br>\n",
    "    $\\nabla_w J(w) = (y - q(s,a;w)) * \\nabla_w q(s,a;w)$ <br><br>\n",
    "    $\\nabla_w J(w) = TD Error * \\nabla_w (W^T \\phi (s,a)) $ <br><br>\n",
    "    Since gradient is with respect to 'w' <br><br>\n",
    "    $\\nabla_w J(w) = TD Error * \\phi (s,a) $ <br><br>\n",
    "    $\\nabla_w J(w) = 5.15 * \\begin{bmatrix} 2 . s \\\\ a \\\\ 0.5 \\end{bmatrix} $ <br><br>\n",
    "    $\\nabla_w J(w) = \\begin{bmatrix} -10.3 \\\\ 0 \\\\ -2.575 \\end{bmatrix} $ <br><br>\n",
    "\n",
    "5. Update the Weights <br><br>\n",
    "    $W_{new} = W - \\alpha \\nabla_w J(w)$ <br><br>\n",
    "    $W_{new} = \\begin{bmatrix} -2 \\\\ 1 \\\\ -1 \\end{bmatrix} - 0.2 * \\begin{bmatrix} -10.3 \\\\ 0 \\\\ -2.575 \\end{bmatrix}$ <br><br>\n",
    "    $W_{new} = \\begin{bmatrix} 0.06 \\\\ 1 \\\\ -0.485 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q3](./images/Q3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given, <br><br>\n",
    "**Actor-Critic algorithm** <br>\n",
    "**Actor:** represents the policy $\\pi_{\\theta}(a|s)$, parameterized by $\\theta$\n",
    "<br>\n",
    "\n",
    "**Critic:** represents the action-state value function $Q_w(s,a)$, parameterized by $W$\n",
    "<br>\n",
    "\n",
    "Use online Actor-Critic\n",
    "\n",
    "1. Critic is updated using the TD_error with a target network using the equation <br><br>\n",
    "$ y = r + \\gamma \\max_{a'} Q_{target}(s',a'; w^-)$ <br><br>\n",
    "$ TD Error = (r + \\gamma \\max_{a'} Q_{target}(s',a'; w^-)) - Q_w(s,a; w)$\n",
    "$ a' = \\pi_{\\theta(a|s)}$\n",
    "$\\nabla_w J(w) = (y - q(s,a;w)) * \\nabla_w Q(s,a;w)$ <br><br>\n",
    "$W_{new} = W + \\alpha * TD Error * \\nabla_w Q(w)$ <br><br>\n",
    "\n",
    "Where, <br><br>\n",
    "        s current state <br><br>\n",
    "        a current action <br><br>\n",
    "        s' next state <br><br>\n",
    "        a' next action <br><br>\n",
    "        w^- is the parameter for Q target network which is updates using $Q_w(s,a)$ for every 'm' steps <br><br>\n",
    "        J(w) is objective function <br><br>\n",
    "        $\\nabla_w J(w)$ is the gradient of the objective function <br><br>\n",
    "        W is the parameter which describes the $Q_w(s,a)$ <br><br>\n",
    "\n",
    "2. Actor is updated using an advantage function $A(s,a)$ <br><br>\n",
    "\n",
    "\n",
    "### Psuedo code\n",
    "```\n",
    "# Define Actor\n",
    "actor = NN(pi(theta))\n",
    "action = actor(state)\n",
    "\n",
    "# Define Critic\n",
    "critic = NN(Q(W))\n",
    "Q = critic(state,action)\n",
    "loss = MSE(y, q)\n",
    "\n",
    "# Define env\n",
    "env\n",
    "\n",
    "episode 0 to num_episodes:\n",
    "    state = env.reset()\n",
    "    steps start_s to end_s or !terminated:\n",
    "        #get an action for state s using pi(theta)\n",
    "        action = actor(state)\n",
    "\n",
    "        # perform the step using action\n",
    "        (state, action, reward, next_state, terminated) = step(action)\n",
    "\n",
    "        # Get the next max action from the policy\n",
    "        next_max_action = actor(next_state)\n",
    "\n",
    "        # Calcualte TD target and TD error\n",
    "        y = reward + gamma * critic(next_state, next_max_action)\n",
    "        x = critic(state,action)\n",
    "        TD_error = y - x\n",
    "\n",
    "        # Update W using\n",
    "        W = W + alpha * TD_error * Divergence(Q_w(state,action))\n",
    "\n",
    "        # Update theta using \n",
    "        theta = theta + \n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
